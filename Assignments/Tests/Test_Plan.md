# Test Plan

## Description of Overall Test Plan

For this project, we will primarily focus on black-box style testing because of our employment of multiple systems not developed by us in our project. To be specific, we are employing mainstream AI models available publically on the internet. As such, we assume their internal working has been thoroughly tested and are sufficiently functional for our purposes. We will also not focus on performance tests as our project is a proof-of-concept and experimental in nature, and so performance optimization is not a priority. We will focus on a unit-testing scaled up to integration-testing format; testing inidividual components of our system followed by testing the interaction between these components.

## Test Case Descriptions

  ### 1 - Stable Diffusion Boot-up Test

  **Test Indication:** Functional, Unit Test
  The purpose of this test is to ensure that Stable Diffusion, the generative AI model for creating image assets, is successfully initialized and running. The test will attempt to execute the script responsible for initializaing Stable Diffusion and then will feed an empty prompt to it. The test will then verify that its associated process is active and running on the computer.
  <br><br>
  **Input:** Execution of the Stable Diffusion Boot Script with an empty prompt.<br>
  **Expected Output:** Actively running process for Stable Diffusion present upon process look-up.

  ### 2 - Blip Image Captioning Model Boot-up Test

  **Test Indication:** Functional, Unit Test
  The purpose of this test is to ensure that the Blip Image Captioning image-to-text AI model is successfully initialized and running. The test will attempt to run the the image_to_text.py script responsible for calling the model with a simple target of a white image. The expected output is a singular text description of the image. The validity or accuracy of the text description returned will not be evaluated.
  <br><br>
  **Input:** Execute of the image_to_text.py script with a blank-white image target.<br>
  **Expected Output:** Any text description returned for the image.

  ### 3 - MiniLlama Boot-up Test

  **Test Indication:** Functional, Unit Test
  The purpose of this test is to ensure that MiniLlama, the large language model leveraged to oversee the background processes of the application, is successfully initialized and running. To achieve this, the model's initialize.py file will be executed and an example empty prompt will be fed into it. The expected output is some response generated by the LLM.
  <br><br>
  **Input:** THe execution of the MiniLlama boot script with an empty string prompt.<br>
  **Expected Output:** Any prompt response by the MiniLlama model.

  ### 4 - ChatGPT API Connectivity Test

  **Test Indication:** Functional, Unit Test
  The purpose of this test is to ensure that the application is successfully able to send requests and receive responses from the ChatGPT API. The test will also verify whether the application has been fed with a valid API key. To achieve this, a test request will be curled to the ChatGPT API, and it will be verified if the response recieved has a status code of 200, indicating success.
  <br><br>
  **Input:** A sample HTTP request curled to the ChatGPT API.<br>
  **Expected Output:** A response with HTTP status code 200.

  ### 5 - Image Pipeline Test

  **Test Indication:** Integration, Functional Test
  The purpose of this test is to ensure that the image generation pipeline, i.e., the image generation from a prompt by Stable Diffusion, to its verbal description by Blip Image Captioning model and vetting by the MiniLlama LLM in order to be made available to the asset list is successful. To acieve this, the prompt of a "golden sword" will be fed to the image generation script. If this pipleline was successful, an image will be made available in the assets folder.
   <br><br>
  **Input:** A sample prompt "golden sword" fed to the image generation pipeline.<br>
  **Expected Output:** New image files added to the Assets folder.

  ### 6 - Asset Placement Test

  **Test Indication:** Functional, Unit Test
  The purpose of this test is to verify whether the asset images are successfully placed on the canvas. This test does not verify if the object was positioned correctly; only that it was placed. To achieve this, an image will be fed into the asset placement queue of a running instance of a map canvas with a central position, and it will be verified when the position tracking instance of MiniLlama is ready to recall objects. It will be asked to recall the object fed to it. If the instance can locate the image, it is considered successful.
  <br><br>
  **Input:** An example image is fed into the asset placement queue.<br>
  **Expected Output:** When the MiniLlama instance for positional tracking is ready to take requests, it successfully recalls the image placed into the queue on the map.

  ### 7 - Asset Placement Accuracy Test

  **Test Indication:** Normal/Functional Test
  The purpose of this test is to verify whether an image with a standard location description can successfully be placed in or near the specified location on the map. For this test, a generic test image will be queued into the asset placement pipeline with the location of "upper-right corner". After the position tracking MiniLlama instance is free to take requests, the location of asset image will be pulled by verifying its coordinates. if the coordinates returned are greater than 75% X and 75% Y (indicating it is near the upper-right corner), the test will be considered a success.
  <br><br>
  **Input:** A test image is fed into the asset placement queue.<br>
  **Expected Output:** A coordinate for the placed image that has at least 75% X and 75% Y in coordinates (depending on the size of the canvas).

  ### 8 - Outdoor Map Creation Test

  **Test Indication**: Normal/Functional Test

  The purpose of this test is to verify whether the system can successfully create and place assets in an outdoor environment. This test aims to assess the capability of the system to handle scenarios related to outdoor landscapes.

  **Input:** A specific prompt related to outdoor environments, such as "sunny meadow" or "dense forest," is fed into the image generation pipeline.
  <br><br>
  **Expected Output:** Asset images depicting outdoor scenes are added to the Assets folder. The MiniLlama instance for positional tracking successfully recalls the placed outdoor images on the map.
  <br>
  Adjust the details based on the specific requirements of your project and the types of outdoor environments you want to test.

  ### 9 - Indoor / Cave Creation Test

  **Test Indication**: Normal/Functional Test
  The purpose of this test is to verify whether the system can successfully create and place assets in an indoor or cave environment. This test aims to assess the capability of the system to handle different environmental scenarios.
  <br><br>
  **Input:** A specific prompt related to indoor or cave environments is fed into the image generation pipeline. The prompt has enumerated n entrances/exits, allowing it to connect to other maps. 
  <br>
  **Expected Output:** Asset images related to indoor or cave walls are added to the Assets folder. The MiniLlama instance for positional tracking successfully recalls the placed images on the map. The spatial LLM assembles the walls without gaps. Overlaps are acceptable, up to 50%. The map has entrances/exits are visible, and not covered by images/assets. 

  #### 10 - Performance Test
  **Test Indication**: Performance Test
  The purpose of this test is to evaluate the performance of the system under different conditions, including load testing and response time analysis.
  <br><br>
  **Input:** The same prompts from each previous test [1-9] will be used. Additionally, another input to be used will be an enumeration of item fills from the range of 0 to 100 with steps of 5, instructing the LLM to place them in equal minimum proximity to each other. 
  <br>
  **Expected Output:** The system should handle the increased load without significant degradation in performance. The system should not exceed 30 seconds for any prompt. Response times should remain within acceptable limits, and the system should not experience crashes or failures.

  
## Test Case Matrix
